{"class_name": "Tokenizer", "config": {"num_words": "100", "filters": "", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 79677, "word_counts": "{}", "word_docs": "{}", "index_docs": "{}", "index_word": "{}", "word_index": "{}"}}
